# Location

Hangzhou International Expo Center

# Important Dates

* [Event SLAM Competition](https://nail-hnu.github.io/EvSLAM/index.html)
  * Start Time: July 1, 2025
  * End Time: September 30, 2025
  * Winners announcement: October 1, 2025
* Workshop date: October 20, 2025


# Context
<div style="text-align: justify">
Event-based cameras are bio-inspired visual sensors that mimic the transient pathway of the human visual system, offering
key advantages (e.g., microsecond temporal resolution and high dynamic range) that hold the potential to revolutionize robot
state estimation and image processing. Since the first commercially available event camera in 2008 and the first Workshop on
Event-based Vision at ICRA 2017, the community has witnessed a surge in event-based/-enhanced solutions for robotics and
computer vision. However, the community is facing a chicken-and-egg dilemma: on one hand, the high price of event cameras
stifles the community growth; on the other hand, the absence of large-scale deployment of event-based solutions discourages
mass production of these cameras. To this end, this workshop is dedicated to event-based vision, with a particular focus on its
development in state estimation and image processing.
</div>
<br>
<div style="text-align: justify">
This workshop builds on the tradition of inviting pioneering figures in the community as speakers, while also serving as a bridge between international/domestic start-ups and academia. It aims to promote discussions on identifying roadblocks that hinder progress in the field and foster collaborative solutions to overcome these barriers. Besides, the first-ever Event-based SLAM Challenge will be held in this workshop. This challenge seeks to benchmark state-of-the-art algorithms, encourage innovation in event-driven/-enhanced approaches, and push the boundaries of what is achievable in real-time ultra-frame-rate state estimation for high-speed robots. As a whole, this workshop will place a strong emphasis on the reproducibility of research findings in real-world scenarios and their tangible impact on advancing robotics technology
</div>
<br>

<!-- All invited speakers are confirmed. -->
# Program

| **Time**             | **Speaker**                         | **Topic/Title**                                                                 |
|----------------------|--------------------------------------|---------------------------------------------------------------------------------|
| 13:30pm–13:40pm       | Organizers                          | Welcome Talk – Introduction of the Workshop                                     |
| 13:40pm–14:00pm       | Tobias Fischer                     | *Localizing Faster and Sooner: Adventures in Event Cameras and Spiking Neural Networks* |
| 14:00pm–14:20pm       | Yulia Sandamirskaya                 | *Neuromorphic Computing: From Theory to Applications*                           |
| 14:20pm–14:40pm       | Jinshan Pan                         | *Event-Based Imaging: Advancements in Enhancing Visual Perception under Challenging Conditions* |
| 14:40pm–15:00pm       | Kuk-Jin Yoon                        | *Multi-Modal Fusion in Computer Vision: Leveraging Event Data for Enhanced Object Detection and Scene Understanding* |
| 15:00pm–15:20pm       | Lei Yu                              | *Integrating Asynchronous Event Data with New Deep Learning Models: Challenges, Techniques, and Future Directions* |
| 15:20pm–15:40pm       | -                                   | Tea Break                                                                       |
| 15:40pm–15:55pm       | Ning Qiao (CEO of SynSense)         | *Neuromorphic Sensing and Computing Empowering Industrial Intelligence*         |
| 15:55pm–16:10pm       | Min Liu (CEO of Dvsense)            | *Revolutionizing Vision with Event Cameras: Insights from an Industry Startup* |
| 16:10pm–16:20pm       | Organizers                          | Intro of Event-based SLAM Challenge: background, setup                          |
| 16:20pm–16:30pm       | Organizers                          | Awards Ceremony                                                                 |
| 16:30pm–16:50pm       | Winner                              | Event SLAM Challenge Winner Presentation                                        |
| 16:50pm–17:30pm       | Panelists                           | *Community Dilemma: High Event Camera Costs vs. Limited Adoption Hindering Growth and Mass Production* |
| 17:30pm              | -                                   | End                                                                             |


**Note**: All times are in the local time zone of IROS 2025 (Beijing).


# Speakers  
<!-- copy paste this for each speaker
<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/example.jpg" alt="Image" width="150" height="150" />
    </div>
      <div class="text">
        <h3>Title of presentation</h3>
        <strong>Name</strong><br/>
        <em>Affiliation</em>  <br/>
        <a href="">Personal website</a>
      </div>
</div>

<div style="clear:left;">
</div>
<br>
end speaker1-->

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/tobias_fischer.webp" alt="Image" width="140" />
    </div>      
    <div class="text">
      <h3>Localizing Faster and Sooner: Adventures in Event Cameras and Spiking Neural Networks</h3>
      <strong>Tobias Fischer, Queensland University of Technology</strong><br/>
      <a href="https://www.tobiasfischer.info/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>Knowing your location has long been fundamental to robotics and has driven major technological advances from industry to academia. Despite significant research advances, critical challenges to enduring deployment remain, including deploying these advances on resource-constrained robots and providing robust localisation capabilities in GPS-denied challenging environments. This talk explores Visual Place Recognition (VPR), which is the ability to recognise previously visited locations using only visual data. I will demonstrate how energy-efficient neuromorphic approaches using event-based cameras and spiking neural networks can provide low-power edge devices with location information with superior energy efficiency, adaptability, and data efficiency.</p>
    </details>    
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/yulia.png" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Neuromorphic Computing: From Theory to Applications</h3>
      <strong>Yulia Sandamirskaya, Zurich University of Applied Sciences</strong><br/>
      <a href="https://sandamirskaya.eu/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/jspan_photo.jpg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Event-Based Imaging: Advancements in Enhancing Visual Perception under Challenging Conditions</h3>
      <strong>Jinshan Pan, Nanjing University of Science and Technology</strong><br/>
      <a href="https://jspan.github.io/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/kukjin_yoon.jpeg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Multi-Modal Fusion in Computer Vision: Leveraging Event Data for Enhanced Object Detection and Scene Understanding</h3>
      <strong>Kuk-Jin Yoon, Korea Advanced Institute of Science & Technology (KAIST)</strong><br/>
      <a href="http://vi.kaist.ac.kr/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/lei_yu.jpeg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Integrating Asynchronous Event Data with New Deep Learning Models: Challenges, Techniques, and Future Directions</h3>
      <strong>Lei Yu, Wuhan University</strong><br/>
      <a href="http://dvs-whu.cn/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/ning_qiao.jpeg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Neuromorphic Sensing and Computing Empowering Industrial Intelligence</h3>
      <strong>Ning Qiao, CEO of SynSense</strong><br/>
      <a href="https://scholar.google.com/citations?user=e7FIdOMAAAAJ&hl=en">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/min_liu.jpeg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Revolutionizing Vision with Event Cameras: Insights from an Industry Startup</h3>
      <strong>Min Liu, CEO of Dvsense</strong><br/>
      <a href="https://scholar.google.com/citations?user=9YYkL8kAAAAJ&hl=en">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>    
</div>

<div style="clear:left;">
</div>
<br>



# Event SLAM Competition

We introduce a benchmarking framework for the task of **event-based state estimation**, featuring:

* **A novel dataset** that complements missing characteristics in existing ones
* **A novel evaluation metric** that can fairly measure the operation boundaries of event-based solutions

This framework is instantiated through an **IROS 2025 Workshop Competition** that benchmarks state-of-the-art methods, yielding insights into optimal architectures and persistent challenges. 

Please visit the competition websites for more details: [Overview](https://nail-hnu.github.io/EvSLAM/index.html) and [Submission](https://www.codabench.org/competitions/9407/)

Any questions about the competition can be directed at <a href="mailto:junkainiu@hnu.edu.cn">junkainiu@hnu.edu.cn</a>.

# Workshop Organizers
<div style="text-align: center;">
  <table style="margin: 0 auto; border-collapse: collapse; border: none; cellpadding: 0; cellspacing: 0;">
    <tr>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/yizhou.jpg" alt="Yi Zhou" style="max-width: auto; height: 100px;"><br>
        <strong>Yi Zhou</strong><br>
        <em>Hunan University</em><br>
        <a href="https://sites.google.com/view/zhouyi-joey">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/jianhao_jiao.jpeg" alt="Jianhao Jiao" style="max-width: auto; height: 100px;"><br>
        <strong>Jianhao Jiao</strong><br>
        <em>UCL</em><br>
        <a href="https://gogojjh.github.io">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/yifu_wang.jpg" alt="Yifu Wang" style="max-width: auto; height: 100px;"><br>
        <strong>Yifu Wang</strong><br>
        <em>Vertex Lab</em><br>
        <a href="https://1fwang.github.io">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/boxin.jpg" alt="Boxin Shi" style="max-width: auto; height: 100px;"><br>
        <strong>Boxin Shi</strong><br>
        <em>Peking University</em><br>
        <a href="https://camera.pku.edu.cn">Personal website</a>
      </td>
    </tr>
    <tr>
      <td style="width: 200px; vertical-align: top; height: 140px; border: none; padding: 5px;">
        <img src="images/organizers/liyuan_pan.jpg" alt="Liyuan Pan" style="max-width: auto; height: 100px;"><br>
        <strong>Liyuan Pan</strong><br>
        <em>Beijing Institute of Technology</em><br>
        <a href="https://bitsslab.github.io/">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: 140px; border: none; padding: 5px;">
        <img src="images/organizers/laurent_kneip.jpeg" alt="Laurent Kneip" style="max-width: auto; height: 100px;"><br>
        <strong>Laurent Kneip</strong><br>
        <em>ShanghaiTech University</em><br>
        <a href="https://mpl.sist.shanghaitech.edu.cn/">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: 140px; border: none; padding: 5px;">
        <img src="images/organizers/richard_hartley.jpeg" alt="Richard Hartley" style="max-width: auto; height: 100px;"><br>
        <strong>Richard Hartley</strong><br>
        <em>Australian National University</em><br>
        <a href="https://comp.anu.edu.au/people/richard-hartley/">Personal website</a>
      </td>
    </tr>
  </table>
</div>


# Challenge Organizers
<div style="text-align: center;">
  <table style="margin: 0 auto; border-collapse: collapse; border: none; cellpadding: 0; cellspacing: 0;">
    <tr>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/njk.jpg" alt="Junkai Niu" style="max-width: auto; height: 100px;"><br>
        <strong>Junkai Niu</strong><br>
        <em>HNU, NAIL Lab</em><br>
        <a href="https://scholar.google.com/citations?user=EpIxnIEAAAAJ&hl=zh-CN">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/zs.jpg" alt="Sheng Zhong" style="max-width: auto; height: 100px;"><br>
        <strong>Sheng Zhong</strong><br>
        <em>HNU, NAIL Lab</em><br>
        <a href="https://nail-hnu.github.io/EvSLAM/images/index/zs.jpg">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/sunkaizhen.jpg" alt="Kaizhen Sun" style="max-width: auto; height: 100px;"><br>
        <strong>Kaizhen Sun</strong><br>
        <em>HNU, NAIL Lab</em><br>
        <a href="https://scholar.google.com/citations?user=ZvVrufAAAAAJ&hl=zh-CN&oi=ao">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/yizhou.jpg" alt="Yi Zhou" style="max-width: auto; height: 100px;"><br>
        <strong>Yi Zhou</strong><br>
        <em>HNU, NAIL Lab</em><br>
        <a href="https://sites.google.com/view/zhouyi-joey">Personal website</a>
      </td>
    </tr>
    <tr>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/David.png" alt="Davide Scaramuzza" style="max-width: auto; height: 100px;"><br>
        <strong>Davide Scaramuzza</strong><br>
        <strong>(Advisory Board)</strong><br>
        <em>UZH, RPG Lab</em><br>
        <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/gg.jpg" alt="Guillermo Gallego" style="max-width: auto; height: 100px;"><br>
        <strong>Guillermo Gallego</strong><br>
        <strong>(Advisory Board)</strong><br>
        <em>TU Berlin, Robotic Interactive Perception Lab</em><br>
        <a href="https://www.digital-future.berlin/en/about-us/professors/prof-dr-guillermo-gallego/">Personal website</a>
      </td>
    </tr>
  </table>
</div>

# Contact

|  | **Email** | **Responsibility** |
|----------|-----------|-------------------|
| Prof.Yi Zhou | [eeyzhou(at)hnu(dot)edu(dot)cn](mailto:eeyzhou@hnu.edu.cn) | General workshop inquiries |
| Dr.Jianhao Jiao | [jiaojh1994(at))gmail(dot)com](mailto:jiaojh1994@gmail.com) | Website and advertising-related questions |
| Dr.Yifu Wang | [usasuper(at)126(dot)com](mailto:usasuper@126.com) | Speaker information and program details |


<!-- TBD -->
<!-- ## Competition Organizers
<div style="text-align: center;">
  <table style="margin: 0 auto; border-collapse: collapse; border: none; cellpadding: 0; cellspacing: 0;">
    <tr>
      <td style="width: 80px; vertical-align: top; height: 120px; border: none;">
        <img src="images/workshop/jan_faigl.png" alt="Jan Faigl" style="max-width: 80px; height: auto;"><br><strong>Jan Faigl</strong>
      </td>
      <td style="width: 80px; vertical-align: top; height: 120px; border: none;">
        <img src="images/workshop/gerald_steinbauer_wagner.jpg" alt="Gerald Steinbauer-Wagner" style="max-width: 80px; height: auto;"><br><strong>Gerald Steinbauer-Wagner</strong>
      </td>
      <td style="width: 80px; vertical-align: top; height: 120px; border: none;">
        <img src="images/workshop/milos_pragr.jpg" alt="Miloš Prágr" style="max-width: 80px; height: auto;"><br><strong>Miloš Prágr</strong>
      </td>
      <td style="width: 80px; vertical-align: top; height: 120px; border: none;">
        <img src="images/workshop/raphael_hagmanns.jpg" alt="Raphael Hagmanns" style="max-width: 80px; height: auto;"><br><strong>Raphael Hagmanns</strong>
      </td>
    </tr>
    <tr>
      <td style="width: 80px; vertical-align: top; height: 120px; border: none;">
        <img src="images/workshop/miguel_granero.jpeg" alt="Miguel Granero" style="max-width: 80px; height: auto;"><br><strong>Miguel Granero</strong>
      </td>
      <td style="width: 80px; vertical-align: top; height: 120px; border: none;">
        <img src="images/workshop/vladimir_kubelka.jpg" alt="Vladimír Kubelka" style="max-width: 80px; height: auto;"><br><strong>Vladimír Kubelka</strong>
      </td>
      <td style="width: 80px; vertical-align: top; height: 120px; border: none;">
        <img src="images/workshop/peter_mortimer.jpg" alt="Peter Mortimer" style="max-width: 80px; height: auto;"><br><strong>Peter Mortimer</strong>
      </td>
    </tr>
  </table>
</div> -->


<!-- # Call for papers

The workshop topics include, but are not limited to:

- Agriculture
- Construction
- Forestry
- Healthcare
- Intelligent Transportation Systems
- Marine Robotics
- Mining
- Search and rescue
- Space exploration

## Submission guidelines

FR workshop accepts contributions based on the following criteria:

- The submission should be from 2 to 8 pages long. The paper should follow the [IEEE RAS template](http://ras.papercept.net/conferences/support/tex.php).
- The review process is single-blind.
- Submissions should contain a clear focus on field robotics and feature lessons learned and/or field experience reports.
- We welcome prospective and conceptual papers as well.
- The papers should be submitted on [Microsoft CMT](https://cmt3.research.microsoft.com/FRICRA2025).
- Accepted papers will be available on the workshop website. The authors of accepted papers will be invited to present their results in a poster session during the workshop. -->