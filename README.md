# Location

Hangzhou International Expo Center, Room 301

# Important Dates

* [Event SLAM Challenge](https://nail-hnu.github.io/EvSLAM/index.html)
  * Start Time: July 1, 2025
  * End Time: September 30, 2025
  * Winners announcement: October 1, 2025
  * **Awards** - First Prize: 3K RMB; Second Prize: 2K RMB; Third Prize: 1K RMB.
* Workshop date: October 20, 2025


# Context
<div style="text-align: justify">
Event-based cameras are bio-inspired visual sensors that mimic the transient pathway of the human visual system, offering
key advantages (e.g., microsecond temporal resolution and high dynamic range) that hold the potential to revolutionize robot
state estimation and image processing. Since the first commercially available event camera in 2008 and the first Workshop on
Event-based Vision at ICRA 2017, the community has witnessed a surge in event-based/-enhanced solutions for robotics and
computer vision. However, the community is facing a chicken-and-egg dilemma: on one hand, the high price of event cameras
stifles the community growth; on the other hand, the absence of large-scale deployment of event-based solutions discourages
mass production of these cameras. To this end, this workshop is dedicated to event-based vision, with a particular focus on its
development in state estimation and image processing.
</div>
<br>
<div style="text-align: justify">
This workshop builds on the tradition of inviting pioneering figures in the community as speakers, while also serving as a bridge between international/domestic start-ups and academia. It aims to promote discussions on identifying roadblocks that hinder progress in the field and foster collaborative solutions to overcome these barriers. Besides, the first-ever Event-based SLAM Challenge will be held in this workshop. This challenge seeks to benchmark state-of-the-art algorithms, encourage innovation in event-driven/-enhanced approaches, and push the boundaries of what is achievable in real-time ultra-frame-rate state estimation for high-speed robots. As a whole, this workshop will place a strong emphasis on the reproducibility of research findings in real-world scenarios and their tangible impact on advancing robotics technology
</div>
<br>

<!-- All invited speakers are confirmed. -->
# Program

| **Time** | **Speaker** | **Topic/Title** |
| :--- | :--- | :--- |
| **13:00pm–13:05pm** | Organizers | Welcome Talk - Introduction of the Workshop |
| **13:05pm–13:30pm** | Prof. Tobias Fischer | Localizing Faster and Sooner: Adventures in Event Cameras and Spiking Neural Networks |
| **13:30pm–13:55pm** | Prof. Yuchao Dai | Event Camera Vision: Motion Perception and Generation |
| **13:55pm–14:20pm** | Prof. Yu Lei | Integrating Asynchronous Event Data with New Deep Learning Models: Challenges, Techniques, and Future Directions |
| **14:20pm–14:40pm** | Dr. Min Liu (CEO of Dvsense) | Revolutionizing Vision with Event Cameras: Insights from an Industry Startup |
| **14:40pm–15:00pm** | Dr. Ning Qiao (CEO of SynSense) | Neuromorphic Sensing and Computing Empowering Industrial Intelligence |
| **15:00pm–15:30pm** | - | Tea Break |
| **15:30pm–15:55pm** | Prof. Jinshan Pan | Event-Based Imaging: Advancements in Enhancing Visual Perception under Challenging Conditions |
| **15:55pm–16:05pm** | Organizers | Intro of Event-based SLAM Challenge: Background, Setup |
| **16:05pm–16:10pm** | Organizers | Awards Ceremony |
| **16:10pm–17:10pm** | Winner | Winner Presentation |
| **17:10pm–17:40pm** | Panelists | Community Dilemma: High Event Camera Costs vs. Limited Adoption Hindering Growth and Mass Production |
| **17:40pm** | - | End |

**Note**: All times are in the local time zone of IROS 2025 (Beijing).


# Speakers  
<!-- copy paste this for each speaker
<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/example.jpg" alt="Image" width="150" height="150" />
    </div>
      <div class="text">
        <h3>Title of presentation</h3>
        <strong>Name</strong><br/>
        <em>Affiliation</em>  <br/>
        <a href="">Personal website</a>
      </div>
</div>

<div style="clear:left;">
</div>
<br>
end speaker1-->

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/tobias_fischer.webp" alt="Image" width="140" />
    </div>      
    <div class="text">
      <h3>Localizing Faster and Sooner: Adventures in Event Cameras and Spiking Neural Networks</h3>
      <strong>Tobias Fischer, Queensland University of Technology</strong><br/>
      <a href="https://www.tobiasfischer.info/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>Knowing your location has long been fundamental to robotics and has driven major technological advances from industry to academia. Despite significant research advances, critical challenges to enduring deployment remain, including deploying these advances on resource-constrained robots and providing robust localisation capabilities in GPS-denied challenging environments. This talk explores Visual Place Recognition (VPR), which is the ability to recognise previously visited locations using only visual data. I will demonstrate how energy-efficient neuromorphic approaches using event-based cameras and spiking neural networks can provide low-power edge devices with location information with superior energy efficiency, adaptability, and data efficiency.</p>
    </details>    
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/yuchao_dai.jpg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Event Camera Vision: Motion Perception and Generation</h3>
      <strong>Yuchao Dai, Northwestern Polytechnical University</strong><br/>
      <a href="https://sites.google.com/site/daiyuchao/Home?authuser=0/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>As a new type of neuromorphic vision sensor, the event camera asynchronously responds to pixel-level brightness changes, breaking through the limitations of traditional frame-based cameras in high-speed motion and high-dynamic-range scenarios. Event cameras show great potential in fields such as autonomous driving, robot navigation, military defense, deep space exploration, and high-speed industrial inspection. This talk focuses on our research group's work in event camera-based motion perception and generation, covering sub-tasks such as 2D and 3D motion estimation, long-term point trajectory tracking, moving object tracking and segmentation, video frame generation, and novel view synthesis. The goal is to overcome existing perception bottlenecks of frame-based cameras and demonstrate the potential of event cameras for perception and generation in complex dynamic scenes.</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/lei_yu.jpeg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>How to Integrate Asynchronous Events in Our Imaging Pipeline?</h3>
      <strong>Lei Yu, Wuhan University</strong><br/>
      <a href="http://dvs-whu.cn/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>We explore the integration of asynchronous event-based vision with traditional imaging pipelines to enhance visual perception capabilities. Event cameras, which capture pixel-level brightness changes asynchronously with microsecond temporal resolution, offer significant advantages over conventional frame-based cameras in challenging scenarios such as high-speed motion, extreme lighting conditions, and power-constrained environments. We present novel methodologies for seamlessly incorporating event data into existing imaging systems, including aperture synthesis, auto-focusing, shutter control, and post-processing fusion. Our approach demonstrates substantial improvements across all components of the imaging system and exhibits significant potential for downstream tasks including tracking and scene reconstruction, particularly in scenarios where traditional cameras struggle. We will discuss the key challenges and future perspectives for developing next-generation computer vision systems that can leverage the complementary strengths of both event-based and frame-based sensing modalities.</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/min_liu.jpeg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Revolutionizing Vision with Event Cameras: Insights from an Industry Startup</h3>
      <strong>Min Liu, CEO of Dvsense</strong><br/>
      <a href="https://scholar.google.com/citations?user=9YYkL8kAAAAJ&hl=en">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>    
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/ning_qiao.jpeg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Neuromorphic Sensing and Computing Empowering Industrial Intelligence</h3>
      <strong>Ning Qiao, CEO of SynSense</strong><br/>
      <a href="https://scholar.google.com/citations?user=e7FIdOMAAAAJ&hl=en">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/jspan_photo.jpg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Event-Based Imaging: Advancements in Enhancing Visual Perception under Challenging Conditions</h3>
      <strong>Jinshan Pan, Nanjing University of Science and Technology</strong><br/>
      <a href="https://jspan.github.io/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>
<!--
<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/yulia.png" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Neuromorphic Computing: From Theory to Applications</h3>
      <strong>Yulia Sandamirskaya, Zurich University of Applied Sciences</strong><br/>
      <a href="https://sandamirskaya.eu/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>

<div class="container">
    <div class="image">
    <img style="float:left;padding-right:10px;padding-bottom:10px" 
         align='middle'
         src="images/speakers/kukjin_yoon.jpeg" alt="Image" width="140" />
    </div>
    <div class="text">
      <h3>Multi-Modal Fusion in Computer Vision: Leveraging Event Data for Enhanced Object Detection and Scene Understanding</h3>
      <strong>Kuk-Jin Yoon, Korea Advanced Institute of Science & Technology (KAIST)</strong><br/>
      <a href="http://vi.kaist.ac.kr/">Personal website</a>
    </div>
    <details>
      <summary>Abstact</summary>
      <p>TBD</p>
    </details>        
</div>
<div style="clear:left;">
</div>
<br>
-->
<div style="clear:left;">
</div>
<br>

# Event SLAM Challenge

We introduce a benchmarking framework for the task of **event-based state estimation**, featuring:

* **A novel dataset** that complements missing characteristics in existing ones
* **A novel evaluation metric** that can fairly measure the operation boundaries of event-based solutions

This framework is instantiated through an **IROS 2025 Workshop Challenge** that benchmarks state-of-the-art methods, yielding insights into optimal architectures and persistent challenges. 

Please visit the challenge websites for more details: [Overview](https://nail-hnu.github.io/EvSLAM/index.html) and [Submission](https://www.codabench.org/competitions/9407/).

**Cash Awards** - First Prize: 3K RMB; Second Prize: 2K RMB; Third Prize: 1K RMB.

Any questions about the challenge can be directed at <a href="mailto:junkainiu@hnu.edu.cn">junkainiu@hnu.edu.cn</a>.

# Workshop Organizers
<div style="text-align: center;">
  <table style="margin: 0 auto; border-collapse: collapse; border: none; cellpadding: 0; cellspacing: 0;">
    <tr>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/yizhou.jpg" alt="Yi Zhou" style="max-width: auto; height: 100px;"><br>
        <strong>Yi Zhou</strong><br>
        <em>Hunan University</em><br>
        <a href="https://sites.google.com/view/zhouyi-joey">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/jianhao_jiao.jpeg" alt="Jianhao Jiao" style="max-width: auto; height: 100px;"><br>
        <strong>Jianhao Jiao</strong><br>
        <em>UCL</em><br>
        <a href="https://gogojjh.github.io">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/yifu_wang.jpg" alt="Yifu Wang" style="max-width: auto; height: 100px;"><br>
        <strong>Yifu Wang</strong><br>
        <em>Vertex Lab</em><br>
        <a href="https://1fwang.github.io">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/boxin.jpg" alt="Boxin Shi" style="max-width: auto; height: 100px;"><br>
        <strong>Boxin Shi</strong><br>
        <em>Peking University</em><br>
        <a href="https://camera.pku.edu.cn">Personal website</a>
      </td>
    </tr>
    <tr>
      <td style="width: 200px; vertical-align: top; height: 140px; border: none; padding: 5px;">
        <img src="images/organizers/liyuan_pan.jpg" alt="Liyuan Pan" style="max-width: auto; height: 100px;"><br>
        <strong>Liyuan Pan</strong><br>
        <em>Beijing Institute of Technology</em><br>
        <a href="https://bitsslab.github.io/">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: 140px; border: none; padding: 5px;">
        <img src="images/organizers/laurent_kneip.jpeg" alt="Laurent Kneip" style="max-width: auto; height: 100px;"><br>
        <strong>Laurent Kneip</strong><br>
        <em>ShanghaiTech University</em><br>
        <a href="https://mpl.sist.shanghaitech.edu.cn/">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: 140px; border: none; padding: 5px;">
        <img src="images/organizers/richard_hartley.jpeg" alt="Richard Hartley" style="max-width: auto; height: 100px;"><br>
        <strong>Richard Hartley</strong><br>
        <em>Australian National University</em><br>
        <a href="https://comp.anu.edu.au/people/richard-hartley/">Personal website</a>
      </td>
    </tr>
  </table>
</div>


# Challenge Organizers
<div style="text-align: center;">
  <table style="margin: 0 auto; border-collapse: collapse; border: none; cellpadding: 0; cellspacing: 0;">
    <tr>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/njk.jpg" alt="Junkai Niu" style="max-width: auto; height: 100px;"><br>
        <strong>Junkai Niu</strong><br>
        <em>HNU, NAIL Lab</em><br>
        <a href="https://scholar.google.com/citations?user=EpIxnIEAAAAJ&hl=zh-CN">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/zs.jpg" alt="Sheng Zhong" style="max-width: auto; height: 100px;"><br>
        <strong>Sheng Zhong</strong><br>
        <em>HNU, NAIL Lab</em><br>
        <a href="https://nail-hnu.github.io/EvSLAM/images/index/zs.jpg">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/sunkaizhen.jpg" alt="Kaizhen Sun" style="max-width: auto; height: 100px;"><br>
        <strong>Kaizhen Sun</strong><br>
        <em>HNU, NAIL Lab</em><br>
        <a href="https://scholar.google.com/citations?user=ZvVrufAAAAAJ&hl=zh-CN&oi=ao">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/yizhou.jpg" alt="Yi Zhou" style="max-width: auto; height: 100px;"><br>
        <strong>Yi Zhou</strong><br>
        <em>HNU, NAIL Lab</em><br>
        <a href="https://sites.google.com/view/zhouyi-joey">Personal website</a>
      </td>
    </tr>
    <tr>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/David.png" alt="Davide Scaramuzza" style="max-width: auto; height: 100px;"><br>
        <strong>Davide Scaramuzza</strong><br>
        <strong>(Advisory Board)</strong><br>
        <em>UZH, RPG Lab</em><br>
        <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Personal website</a>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/gg.jpg" alt="Guillermo Gallego" style="max-width: auto; height: 100px;"><br>
        <strong>Guillermo Gallego</strong><br>
        <strong>(Advisory Board)</strong><br>
        <em>TU Berlin, Robotic Interactive Perception Lab</em><br>
        <a href="https://www.digital-future.berlin/en/about-us/professors/prof-dr-guillermo-gallego/">Personal website</a>
      </td>
    </tr>
  </table>
</div>

# Sponsor
<div style="text-align: center;">
  <table style="margin: 0 auto; border-collapse: collapse; border: none; cellpadding: 0; cellspacing: 0;">
    <tr>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/logo_synsense.png" alt="SyncSense" style="max-width: auto; height: 100px;"><br>
        <strong>SyncSense</strong><br>
      </td>
      <td style="width: 200px; vertical-align: top; height: auto; border: none; padding: 5px;">
        <img src="images/organizers/logo_inivation.jpg" alt="IniVation" style="max-width: auto; height: 100px;"><br>
        <strong>IniVation</strong><br>
      </td>
    </tr>
  </table>
</div>

# Contact

|  | **Email** | **Responsibility** |
|----------|-----------|-------------------|
| Prof.Yi Zhou | [eeyzhou(at)hnu(dot)edu(dot)cn](mailto:eeyzhou@hnu.edu.cn) | General workshop inquiries |
| Dr.Jianhao Jiao | [jiaojh1994(at))gmail(dot)com](mailto:jiaojh1994@gmail.com) | Website and advertising-related questions |
| Dr.Yifu Wang | [usasuper(at)126(dot)com](mailto:usasuper@126.com) | Speaker information and program details |

